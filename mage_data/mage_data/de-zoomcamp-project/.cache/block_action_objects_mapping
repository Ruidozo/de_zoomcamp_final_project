{"block_file": {"data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_exporters/postgres_tester.py:data_exporter:python:postgres tester": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_postgres(df, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a PostgreSQL database.\n    \"\"\"\n    import pandas as pd\n\n    # Convert to DataFrame if input is a list\n    if isinstance(df, list):\n        print(\"Input is a list. Converting to DataFrame.\")\n        df = pd.DataFrame(df)\n\n    print(f\"Input content: {df}\")\n    print(f\"Input type: {type(df)}\")\n\n\n    # Validate DataFrame\n    if df is None or df.empty:\n        raise ValueError(\"The DataFrame is empty or undefined. Cannot export to PostgreSQL.\")\n\n    print(f\"DataFrame shape: {df.shape}\")\n    print(f\"DataFrame columns: {df.columns}\")\n    print(f\"DataFrame preview:\\n{df.head()}\")\n\n    schema_name = 'public'\n    table_name = 'real_estate_data'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        loader.export(\n            df,\n            schema_name,\n            table_name,\n            index=False,\n            if_exists='replace',\n        )\n    print(f\"Data exported to PostgreSQL table '{schema_name}.{table_name}'.\")\n", "file_path": "data_exporters/postgres_tester.py", "language": "python", "type": "data_exporter", "uuid": "postgres_tester"}, "data_exporters/gcs_tester.py:data_exporter:python:gcs tester": {"content": "from mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.settings.repo import get_repo_path\nfrom pandas import DataFrame\nfrom os import path\n\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_google_cloud_storage(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Export data to a Google Cloud Storage bucket, creating the bucket if it does not exist.\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    bucket_name = 'mage_test_setup'\n    object_key = 'real_estate_data.csv'\n\n    gcs = GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile))\n\n    # Ensure the bucket exists\n    def ensure_bucket_exists(bucket_name):\n        client = gcs.client\n        try:\n            bucket = client.get_bucket(bucket_name)\n            print(f\"Bucket '{bucket_name}' already exists.\")\n        except Exception:\n            bucket = client.bucket(bucket_name)\n            bucket = client.create_bucket(bucket, location=\"europe-west2\")\n            print(f\"Bucket '{bucket_name}' created in location 'europe-west2'.\")\n\n    # Ensure the bucket exists before exporting\n    ensure_bucket_exists(bucket_name)\n\n    # Export the DataFrame to the GCS bucket\n    gcs.export(\n        df,\n        bucket_name,\n        object_key,\n    )\n\n    print(f\"Data exported to GCS bucket '{bucket_name}' at '{object_key}'.\")\n\n    # List all buckets in the project\n    print(\"\\nListing all GCS buckets:\")\n    for bucket in gcs.client.list_buckets():\n        print(f\"- {bucket.name}\")\n", "file_path": "data_exporters/gcs_tester.py", "language": "python", "type": "data_exporter", "uuid": "gcs_tester"}, "data_loaders/api_test_call.py:data_loader:python:api test call": {"content": "import requests\nimport pandas as pd\nimport os\nfrom dotenv import load_dotenv\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n# Load environment variables from the .env file\nload_dotenv('/home/src/.env')\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Load data from the Idealista API using a POST request.\n    \"\"\"\n    # Step 1: Get OAuth Token\n    def get_access_token():\n        url = \"https://api.idealista.com/oauth/token\"\n        headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n        data = {\n            \"grant_type\": \"client_credentials\",\n            \"scope\": \"read\",\n            \"client_id\": os.getenv('CLIENT_ID'),  # Corrected syntax: Keys must be strings\n            \"client_secret\": os.getenv('CLIENT_SECRET'),  # Corrected syntax\n        }\n        response = requests.post(url, headers=headers, data=data)\n        response.raise_for_status()  # Raise an error for bad responses\n        return response.json()[\"access_token\"]\n\n    # Step 2: Make the API call\n    def fetch_real_estate_data(token):\n        url = \"https://api.idealista.com/3.5/es/search\"\n        headers = {\"Authorization\": f\"Bearer {token}\"}\n        data = {\n            \"center\": \"40.430,-3.702\",  # Coordinates (latitude, longitude)\n            \"propertyType\": \"homes\",   # Type of property\n            \"distance\": \"15000\",       # Radius in meters\n            \"operation\": \"sale\",       # Operation type\n        }\n        response = requests.post(url, headers=headers, data=data)\n        response.raise_for_status()  # Raise an error for bad responses\n        return response.json()\n\n    try:\n        # Authenticate and fetch data\n        token = get_access_token()\n        api_response = fetch_real_estate_data(token)\n\n        # Extract the element list into a DataFrame\n        element_list = api_response.get(\"elementList\", [])\n        df = pd.DataFrame(element_list)\n\n        print(\"Fetched data:\", df.head())\n        return df\n    except Exception as e:\n        print(f\"Error during API call: {e}\")\n        return pd.DataFrame()\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Test the output of the data loader block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n    assert not output.empty, 'The output DataFrame is empty'\n    assert 'propertyCode' in output.columns, 'PropertyCode is missing in the output'\n", "file_path": "data_loaders/api_test_call.py", "language": "python", "type": "data_loader", "uuid": "api_test_call"}, "data_loaders/postgres_test.py:data_loader:python:postgres test": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom os import path\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_postgres(*args, **kwargs):\n    \"\"\"\n    Template for loading data from a PostgreSQL database.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#postgresql\n    \"\"\"\n    query = 'your PostgreSQL query'  # Specify your SQL query here\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        return loader.load(query)\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/postgres_test.py", "language": "python", "type": "data_loader", "uuid": "postgres_test"}, "data_loaders/district_loader.py:data_loader:python:district loader": {"content": "import io\nimport pandas as pd\nimport geopandas as gpd\nimport requests\nfrom shapely.geometry import shape\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Load district data from the public API and return it as a GeoPandas DataFrame.\n    \"\"\"\n    url = \"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/georef-portugal-distrito/exports/csv?lang=en&timezone=Europe%2FLondon&use_labels=true&delimiter=%3B\"\n    response = requests.get(url)\n\n    # Check for successful response\n    if response.status_code != 200:\n        raise Exception(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n\n    # Convert response text to DataFrame\n    df = pd.read_csv(io.StringIO(response.text), sep=';')\n\n    # Parse Geo Shape column into geometry\n    df['geometry'] = df['Geo Shape'].apply(lambda x: shape(eval(x)) if pd.notnull(x) else None)\n\n    # Convert to GeoPandas DataFrame\n    gdf = gpd.GeoDataFrame(df, geometry='geometry')\n\n    # Rename relevant columns for clarity\n    gdf = gdf.rename(columns={\n        'Official Name District': 'District Name',\n        'Geo Point': 'Geo Coordinates'\n    })\n\n    # Select only necessary columns\n    gdf = gdf[['District Name', 'Geo Coordinates', 'geometry']]\n\n    # Debugging outputs\n    print(f\"GeoDataFrame shape: {gdf.shape}\")\n    print(f\"GeoDataFrame columns: {gdf.columns}\")\n    print(f\"GeoDataFrame preview:\\n{gdf.head()}\")\n\n    return gdf\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Test the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n    assert not output.empty, 'The output GeoDataFrame is empty'\n    assert 'District Name' in output.columns, 'Expected column \"District Name\" not found in output'\n    assert 'geometry' in output.columns, 'Expected column \"geometry\" not found in output'\n", "file_path": "data_loaders/district_loader.py", "language": "python", "type": "data_loader", "uuid": "district_loader"}, "data_loaders/idealista_loader.py:data_loader:python:idealista loader": {"content": "import requests\nimport pandas as pd\nimport os\nfrom dotenv import load_dotenv\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n# Load environment variables from the .env file\nload_dotenv('/home/src/.env')\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Load data from the Idealista API using a POST request.\n    \"\"\"\n    # Step 1: Get OAuth Token\n    def get_access_token():\n        url = \"https://api.idealista.com/oauth/token\"\n        headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n        data = {\n            \"grant_type\": \"client_credentials\",\n            \"scope\": \"read\",\n            \"client_id\": os.getenv('CLIENT_ID'),  # Corrected syntax: Keys must be strings\n            \"client_secret\": os.getenv('CLIENT_SECRET'),  # Corrected syntax\n        }\n        response = requests.post(url, headers=headers, data=data)\n        response.raise_for_status()  # Raise an error for bad responses\n        return response.json()[\"access_token\"]\n\n    # Step 2: Make the API call\n    def fetch_real_estate_data(token):\n        url = \"https://api.idealista.com/3.5/es/search\"\n        headers = {\"Authorization\": f\"Bearer {token}\"}\n        data = {\n            \"center\": \"40.430,-3.702\",  # Coordinates (latitude, longitude)\n            \"propertyType\": \"homes\",   # Type of property\n            \"distance\": \"15000\",       # Radius in meters\n            \"operation\": \"sale\",       # Operation type\n        }\n        response = requests.post(url, headers=headers, data=data)\n        response.raise_for_status()  # Raise an error for bad responses\n        return response.json()\n\n    try:\n        # Authenticate and fetch data\n        token = get_access_token()\n        api_response = fetch_real_estate_data(token)\n\n        # Extract the element list into a DataFrame\n        element_list = api_response.get(\"elementList\", [])\n        df = pd.DataFrame(element_list)\n\n        print(\"Fetched data:\", df.head())\n        return df\n    except Exception as e:\n        print(f\"Error during API call: {e}\")\n        return pd.DataFrame()\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Test the output of the data loader block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n    assert not output.empty, 'The output DataFrame is empty'\n    assert 'propertyCode' in output.columns, 'PropertyCode is missing in the output'\n", "file_path": "data_loaders/idealista_loader.py", "language": "python", "type": "data_loader", "uuid": "idealista_loader"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "data_loaders/gsc_test.py:data_loader:python:gsc test": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom os import path\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_from_google_cloud_storage(*args, **kwargs):\n    \"\"\"\n    Template for loading data from a Google Cloud Storage bucket.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#googlecloudstorage\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    bucket_name = 'your_bucket_name'\n    object_key = 'your_object_key'\n\n    return GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).load(\n        bucket_name,\n        object_key,\n    )\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/gsc_test.py", "language": "python", "type": "data_loader", "uuid": "gsc_test"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "pipelines/test_autentications/metadata.yaml:pipeline:yaml:test autentications/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - gcs_tester\n  - postgres_tester\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: api_test_call\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: api_test_call\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: GCS tester\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - api_test_call\n  uuid: gcs_tester\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: postgres_tester\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - api_test_call\n  uuid: postgres_tester\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-01-15 10:04:02.902487+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: test_autentications\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: test_autentications\nvariables_dir: /home/src/mage_data/de-zoomcamp-project\nwidgets: []\n", "file_path": "pipelines/test_autentications/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "test_autentications/metadata"}, "pipelines/de_zoomcamp_project/metadata.yaml:pipeline:yaml:de zoomcamp project/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_loaders/idealista_loader.py\n    file_source:\n      path: data_loaders/idealista_loader.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: idealista_loader\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: idealista_loader\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: district_loader\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: district_loader\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-01-15 12:11:43.997748+00:00'\ndata_integration: null\ndescription: The pipeline fetches real estate data from the Idealista API and processes\n  it in parallel to store in two destination\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: de-zoomcamp-project\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags:\n- de-zoomcamp\ntype: python\nuuid: de_zoomcamp_project\nvariables_dir: /home/src/mage_data/de-zoomcamp-project\nwidgets: []\n", "file_path": "pipelines/de_zoomcamp_project/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "de_zoomcamp_project/metadata"}, "pipelines/de_zoomcamp_project/__init__.py:pipeline:python:de zoomcamp project/  init  ": {"content": "", "file_path": "pipelines/de_zoomcamp_project/__init__.py", "language": "python", "type": "pipeline", "uuid": "de_zoomcamp_project/__init__"}, "/home/src/de-zoomcamp-project/data_loaders/district_loader.py:data_loader:python:home/src/de-zoomcamp-project/data loaders/district loader": {"content": "import io\nimport os\nimport pandas as pd\nimport geopandas as gpd\nimport requests\nfrom shapely.geometry import shape\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Load district data from the public API and return it as a GeoPandas DataFrame.\n    \"\"\"\n    url = \"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/georef-portugal-distrito/exports/csv?lang=en&timezone=Europe%2FLondon&use_labels=true&delimiter=%3B\"\n    response = requests.get(url)\n\n    if response.status_code != 200:\n        raise Exception(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n\n    # Read the response data into a Pandas DataFrame\n    df = pd.read_csv(io.StringIO(response.text), sep=';')\n\n    # Parse the 'Geo Shape' column into geometry\n    df['geometry'] = df['Geo Shape'].apply(lambda x: shape(eval(x)) if pd.notnull(x) else None)\n\n    # Convert to GeoPandas DataFrame\n    gdf = gpd.GeoDataFrame(df, geometry='geometry')\n\n    # Rename relevant columns for clarity\n    gdf = gdf.rename(columns={\n        'Official Name District': 'District Name',\n        'Geo Point': 'Geo Coordinates'\n    })\n\n    # Select only necessary columns\n    gdf = gdf[['District Name', 'Geo Coordinates', 'geometry']]\n\n    # Debugging outputs\n    print(f\"GeoDataFrame shape: {gdf.shape}\")\n    print(f\"GeoDataFrame columns: {gdf.columns}\")\n    print(f\"GeoDataFrame preview:\\n{gdf.head()}\")\n\n    # Ensure the output directory exists\n    output_dir = \"/tmp\"\n    if not os.path.exists(output_dir):\n        print(f\"Creating directory: {output_dir}\")\n        os.makedirs(output_dir)\n\n    # Save the GeoDataFrame as a GeoJSON file\n    output_file_path = os.path.join(output_dir, \"district_data.geojson\")\n    gdf.to_file(output_file_path, driver=\"GeoJSON\")\n    print(f\"GeoDataFrame saved to {output_file_path}\")\n\n    return gdf\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Test the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n    assert not output.empty, 'The output GeoDataFrame is empty'\n    assert 'District Name' in output.columns, 'Expected column \"District Name\" not found in output'\n    assert 'geometry' in output.columns, 'Expected column \"geometry\" not found in output'\n", "file_path": "/home/src/de-zoomcamp-project/data_loaders/district_loader.py", "language": "python", "type": "data_loader", "uuid": "district_loader"}}, "custom_block_template": {}, "mage_template": {"data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}