{"block_file": {"data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_exporters/export_sql.py:data_exporter:python:export sql": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_postgres(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Export data to a PostgreSQL database.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#postgresql\n    \"\"\"\n    schema_name = 'public'  # Specify the name of the schema to export data to\n    table_name = 'property_listings'  # Specify the name of the table to export data to\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        loader.export(\n            df,\n            schema_name,\n            table_name,\n            index=False,  # Specifies whether to include index in exported table\n            if_exists='replace',  # Specify resolution policy if table name already exists\n        )", "file_path": "data_exporters/export_sql.py", "language": "python", "type": "data_exporter", "uuid": "export_sql"}, "data_exporters/postgres_tester.py:data_exporter:python:postgres tester": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_postgres(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Export Kaggle dataset to a PostgreSQL database with proper table recreation and schema alignment.\n    \"\"\"\n    import pandas as pd\n\n    # Validate DataFrame\n    if df is None or df.empty:\n        raise ValueError(\"The DataFrame is empty or undefined. Cannot export to PostgreSQL.\")\n\n    print(f\"Original DataFrame columns: {df.columns}\")\n\n    # Clean column names to match PostgreSQL requirements\n    df.columns = (\n        df.columns.str.strip()                 # Remove leading/trailing spaces\n        .str.lower()                           # Convert to lowercase\n        .str.replace(' ', '_')                # Replace spaces with underscores\n        .str.replace(r'[^a-zA-Z0-9_]', '')    # Remove invalid characters\n    )\n    print(f\"Cleaned DataFrame columns: {df.columns}\")\n\n    # Set schema and table name\n    schema_name = 'public'  # Default PostgreSQL schema\n    table_name = 'real_estate_data'\n\n    # Load PostgreSQL configuration\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        # Drop the table if it exists (explicitly)\n        print(f\"Dropping table '{schema_name}.{table_name}' if it exists...\")\n        loader.execute(f\"DROP TABLE IF EXISTS {schema_name}.{table_name} CASCADE;\")\n\n        # Export the DataFrame to PostgreSQL\n        loader.export(\n            df,\n            schema_name=schema_name,\n            table_name=table_name,\n            index=False,        # Do not export the index\n            if_exists='replace' # Replace the table if it already exists\n        )\n    print(f\"Data exported to PostgreSQL table '{schema_name}.{table_name}'.\")\n", "file_path": "data_exporters/postgres_tester.py", "language": "python", "type": "data_exporter", "uuid": "postgres_tester"}, "data_exporters/data_exporter_weely_postgres.py:data_exporter:python:data exporter weely postgres": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_postgres(df: DataFrame, *args, **kwargs) -> None:\n    \"\"\"\n    Export the cleaned DataFrame from the transformer block to a PostgreSQL database.\n    Specify your configuration settings in 'io_config.yaml'.\n    \"\"\"\n    # Define the schema and table names\n    schema_name = 'public'  # Adjust this if you have a custom schema\n    table_name = 'real_estate_data_weekly'  # Name of the table for the exported data\n\n    # Path to the database configuration file\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    # Validate the DataFrame\n    if df is None or df.empty:\n        raise ValueError(\"The DataFrame is empty or undefined. Cannot export to PostgreSQL.\")\n\n    print(f\"Exporting DataFrame to PostgreSQL table '{schema_name}.{table_name}'...\")\n    print(f\"DataFrame columns: {df.columns}\")\n\n    # Export the DataFrame to PostgreSQL\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        loader.export(\n            df,\n            schema_name=schema_name,\n            table_name=table_name,\n            index=False,  # Do not include the DataFrame index in the exported table\n            if_exists='replace',  # Replace the table if it already exists\n        )\n    print(f\"Data exported successfully to '{schema_name}.{table_name}'.\")\n", "file_path": "data_exporters/data_exporter_weely_postgres.py", "language": "python", "type": "data_exporter", "uuid": "data_exporter_weely_postgres"}, "data_exporters/postgres_exp.py:data_exporter:python:postgres exp": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_postgres(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Create a table from the transformed DataFrame and insert all rows.\n    \"\"\"\n    import pandas as pd\n\n    # Validate DataFrame\n    if df is None or df.empty:\n        raise ValueError(\"The DataFrame is empty or undefined. Cannot export to PostgreSQL.\")\n\n    print(f\"DataFrame columns: {df.columns}\")\n\n    schema_name = 'public'\n    table_name = 'real_estate_data'\n\n    # Load PostgreSQL configuration\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n\n\n        # Create the table\n        print(f\"Creating table '{schema_name}.{table_name}'...\")\n        create_table_query = f\"\"\"\n        CREATE TABLE {schema_name}.{table_name} (\n            {', '.join([f\"{col} TEXT\" for col in df.columns])}\n        );\n        \"\"\"\n        print(f\"Executing query: {create_table_query}\")\n        loader.execute(create_table_query)\n        print(f\"Table '{schema_name}.{table_name}' created successfully.\")\n\n\n        print(f\"Data inserted into table '{schema_name}.{table_name}'.\")\n", "file_path": "data_exporters/postgres_exp.py", "language": "python", "type": "data_exporter", "uuid": "postgres_exp"}, "data_exporters/gcs_tester.py:data_exporter:python:gcs tester": {"content": "from mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.settings.repo import get_repo_path\nfrom pandas import DataFrame\nfrom os import path\n\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_google_cloud_storage(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Export data to a Google Cloud Storage bucket, creating the bucket if it does not exist.\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    bucket_name = 'mage_test_setup'\n    object_key = 'real_estate_data.csv'\n\n    gcs = GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile))\n\n    # Ensure the bucket exists\n    def ensure_bucket_exists(bucket_name):\n        client = gcs.client\n        try:\n            bucket = client.get_bucket(bucket_name)\n            print(f\"Bucket '{bucket_name}' already exists.\")\n        except Exception:\n            bucket = client.bucket(bucket_name)\n            bucket = client.create_bucket(bucket, location=\"europe-west2\")\n            print(f\"Bucket '{bucket_name}' created in location 'europe-west2'.\")\n\n    # Ensure the bucket exists before exporting\n    ensure_bucket_exists(bucket_name)\n\n    # Export the DataFrame to the GCS bucket\n    gcs.export(\n        df,\n        bucket_name,\n        object_key,\n    )\n\n    print(f\"Data exported to GCS bucket '{bucket_name}' at '{object_key}'.\")\n\n    # List all buckets in the project\n    print(\"\\nListing all GCS buckets:\")\n    for bucket in gcs.client.list_buckets():\n        print(f\"- {bucket.name}\")\n", "file_path": "data_exporters/gcs_tester.py", "language": "python", "type": "data_exporter", "uuid": "gcs_tester"}, "data_exporters/export_real_estate_data.py:data_exporter:python:export real estate data": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_postgres(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Append the filtered data to the real_estate_data table in PostgreSQL.\n    If the DataFrame is empty, log a message and exit gracefully.\n    \"\"\"\n    schema_name = 'public'  # Adjust this if you have a custom schema\n    table_name = 'real_estate_data'  # Name of the table for the exported data\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    # Check if the DataFrame is empty\n    if df is None or df.empty:\n        print(\"No new data to append. Exiting block.\")\n        return\n\n    print(f\"Appending data to PostgreSQL table '{schema_name}.{table_name}'...\")\n    print(f\"DataFrame contains {len(df)} rows and {len(df.columns)} columns.\")\n\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        loader.export(\n            df,\n            schema_name,\n            table_name,\n            index=False,  # Do not include the index in the exported table\n            if_exists='append',  # Append data to the table\n        )\n    print(f\"Data appended successfully to '{schema_name}.{table_name}'.\")\n", "file_path": "data_exporters/export_real_estate_data.py", "language": "python", "type": "data_exporter", "uuid": "export_real_estate_data"}, "data_exporters/appens_table.py:data_exporter:python:appens table": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_postgres(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a PostgreSQL database.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#postgresql\n    \"\"\"\n    schema_name = 'your_schema_name'  # Specify the name of the schema to export data to\n    table_name = 'your_table_name'  # Specify the name of the table to export data to\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        loader.export(\n            df,\n            schema_name,\n            table_name,\n            index=False,  # Specifies whether to include index in exported table\n            if_exists='replace',  # Specify resolution policy if table name already exists\n        )\n", "file_path": "data_exporters/appens_table.py", "language": "python", "type": "data_exporter", "uuid": "appens_table"}, "data_loaders/energetic_silence.py:data_loader:python:energetic silence": {"content": "import requests\nimport pandas as pd\nimport os\nfrom dotenv import load_dotenv\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n# Load environment variables from the .env file\nload_dotenv('/home/src/.env')\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Load data from the Idealista API using a POST request.\n    \"\"\"\n    # Step 1: Get OAuth Token\n    def get_access_token():\n        url = \"https://api.idealista.com/oauth/token\"\n        headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n        data = {\n            \"grant_type\": \"client_credentials\",\n            \"scope\": \"read\",\n            \"client_id\": os.getenv('CLIENT_ID'),  # Corrected syntax: Keys must be strings\n            \"client_secret\": os.getenv('CLIENT_SECRET'),  # Corrected syntax\n        }\n        response = requests.post(url, headers=headers, data=data)\n        response.raise_for_status()  # Raise an error for bad responses\n        return response.json()[\"access_token\"]\n\n    # Step 2: Make the API call\n    def fetch_real_estate_data(token):\n        url = \"https://api.idealista.com/3.5/es/search\"\n        headers = {\"Authorization\": f\"Bearer {token}\"}\n        data = {\n            \"center\": \"40.430,-3.702\",  # Coordinates (latitude, longitude)\n            \"propertyType\": \"homes\",   # Type of property\n            \"distance\": \"15000\",       # Radius in meters\n            \"operation\": \"sale\",       # Operation type\n        }\n        response = requests.post(url, headers=headers, data=data)\n        response.raise_for_status()  # Raise an error for bad responses\n        return response.json()\n\n    try:\n        # Authenticate and fetch data\n        token = get_access_token()\n        api_response = fetch_real_estate_data(token)\n\n        # Extract the element list into a DataFrame\n        element_list = api_response.get(\"elementList\", [])\n        df = pd.DataFrame(element_list)\n\n        print(\"Fetched data:\", df.head())\n        return df\n    except Exception as e:\n        print(f\"Error during API call: {e}\")\n        return pd.DataFrame()\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Test the output of the data loader block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n    assert not output.empty, 'The output DataFrame is empty'\n    assert 'propertyCode' in output.columns, 'PropertyCode is missing in the output'\n", "file_path": "data_loaders/energetic_silence.py", "language": "python", "type": "data_loader", "uuid": "energetic_silence"}, "data_loaders/kaggle_loader.py:data_loader:python:kaggle loader": {"content": "import os\nimport pandas as pd\nimport kagglehub\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n@data_loader\ndef load_data_from_kaggle(*args, **kwargs):\n    \"\"\"\n    Load data from Kaggle using KaggleHub.\n    \"\"\"\n    try:\n        # Download the latest version of the dataset\n        path = kagglehub.dataset_download(\"luvathoms/portugal-real-estate-2024\")\n        print(\"Path to dataset files:\", path)\n\n        # Print the contents of the directory for debugging\n        print(\"Downloaded files:\", os.listdir(path))\n\n        # Look for a CSV file in the dataset directory\n        dataset_file = None\n        for file in os.listdir(path):\n            if file.endswith(\".csv\"):\n                dataset_file = os.path.join(path, file)\n                break\n        \n        if not dataset_file:\n            raise FileNotFoundError(f\"No CSV file found in dataset directory: {path}\")\n        \n        # Load the data into a pandas DataFrame\n        df = pd.read_csv(dataset_file)\n\n        # Display dataset summary\n        print(f\"Dataset loaded with {df.shape[0]} rows and {df.shape[1]} columns.\")\n        print(df.head())\n        \n        return df\n    except Exception as e:\n        print(f\"Error during data loading: {e}\")\n        return pd.DataFrame()\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Test the output of the Kaggle data loader block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n    assert not output.empty, 'The output DataFrame is empty'", "file_path": "data_loaders/kaggle_loader.py", "language": "python", "type": "data_loader", "uuid": "kaggle_loader"}, "data_loaders/api_test_call.py:data_loader:python:api test call": {"content": "import requests\nimport pandas as pd\nimport os\nfrom dotenv import load_dotenv\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n# Load environment variables from the .env file\nload_dotenv('/home/src/.env')\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Load data from the Idealista API using a POST request.\n    \"\"\"\n    # Step 1: Get OAuth Token\n    def get_access_token():\n        url = \"https://api.idealista.com/oauth/token\"\n        headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n        data = {\n            \"grant_type\": \"client_credentials\",\n            \"scope\": \"read\",\n            \"client_id\": os.getenv('CLIENT_ID'),  # Corrected syntax: Keys must be strings\n            \"client_secret\": os.getenv('CLIENT_SECRET'),  # Corrected syntax\n        }\n        response = requests.post(url, headers=headers, data=data)\n        response.raise_for_status()  # Raise an error for bad responses\n        return response.json()[\"access_token\"]\n\n    # Step 2: Make the API call\n    def fetch_real_estate_data(token):\n        url = \"https://api.idealista.com/3.5/es/search\"\n        headers = {\"Authorization\": f\"Bearer {token}\"}\n        data = {\n            \"center\": \"40.430,-3.702\",  # Coordinates (latitude, longitude)\n            \"propertyType\": \"homes\",   # Type of property\n            \"distance\": \"15000\",       # Radius in meters\n            \"operation\": \"sale\",       # Operation type\n        }\n        response = requests.post(url, headers=headers, data=data)\n        response.raise_for_status()  # Raise an error for bad responses\n        return response.json()\n\n    try:\n        # Authenticate and fetch data\n        token = get_access_token()\n        api_response = fetch_real_estate_data(token)\n\n        # Extract the element list into a DataFrame\n        element_list = api_response.get(\"elementList\", [])\n        df = pd.DataFrame(element_list)\n\n        print(\"Fetched data:\", df.head())\n        return df\n    except Exception as e:\n        print(f\"Error during API call: {e}\")\n        return pd.DataFrame()\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Test the output of the data loader block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n    assert not output.empty, 'The output DataFrame is empty'\n    assert 'propertyCode' in output.columns, 'PropertyCode is missing in the output'\n", "file_path": "data_loaders/api_test_call.py", "language": "python", "type": "data_loader", "uuid": "api_test_call"}, "data_loaders/postgres_test.py:data_loader:python:postgres test": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom os import path\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_postgres(*args, **kwargs):\n    \"\"\"\n    Template for loading data from a PostgreSQL database.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#postgresql\n    \"\"\"\n    query = 'your PostgreSQL query'  # Specify your SQL query here\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        return loader.load(query)\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/postgres_test.py", "language": "python", "type": "data_loader", "uuid": "postgres_test"}, "data_loaders/district_loader.py:data_loader:python:district loader": {"content": "import io\nimport pandas as pd\nimport geopandas as gpd\nimport requests\nfrom shapely.geometry import shape\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Load district data from the public API and return it as a GeoPandas DataFrame.\n    \"\"\"\n    # Fetch data from the API\n    url = \"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/georef-portugal-distrito/exports/csv?lang=en&timezone=Europe%2FLondon&use_labels=true&delimiter=%3B\"\n    response = requests.get(url)\n\n    if response.status_code != 200:\n        raise Exception(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n\n    # Load data into a Pandas DataFrame\n    df = pd.read_csv(io.StringIO(response.text), sep=';')\n\n    # Parse the 'Geo Shape' column into geometry\n    df['geometry'] = df['Geo Shape'].apply(lambda x: shape(eval(x)) if pd.notnull(x) else None)\n\n    # Convert the DataFrame to a GeoPandas GeoDataFrame\n    gdf = gpd.GeoDataFrame(df, geometry='geometry')\n\n    # Rename columns for easier usage\n    gdf = gdf.rename(columns={\n        'Official Name District': 'District Name',\n        'Geo Point': 'Geo Coordinates'\n    })\n\n    # Select only necessary columns\n    gdf = gdf[['District Name', 'Geo Coordinates', 'geometry']]\n\n    # Debugging outputs\n    print(f\"GeoDataFrame shape: {gdf.shape}\")\n    print(f\"GeoDataFrame columns: {gdf.columns}\")\n    print(f\"GeoDataFrame preview:\\n{gdf.head()}\")\n\n    # Redirect output to a temporary file to avoid directory issues\n    valid_output_path = \"/tmp/district_data_temp.geojson\"\n    gdf.to_file(valid_output_path, driver=\"GeoJSON\")\n    print(f\"GeoDataFrame temporarily saved to {valid_output_path}\")\n\n    # Return GeoDataFrame to continue in-memory processing\n    return gdf\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Test the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n    assert not output.empty, 'The output GeoDataFrame is empty'\n    assert 'District Name' in output.columns, 'Expected column \"District Name\" not found in output'\n    assert 'geometry' in output.columns, 'Expected column \"geometry\" not found in output'\n", "file_path": "data_loaders/district_loader.py", "language": "python", "type": "data_loader", "uuid": "district_loader"}, "data_loaders/load_overall_table.py:data_loader:python:load overall table": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom os import path\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_postgres(*args, **kwargs):\n    \"\"\"\n    Template for loading data from a PostgreSQL database.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#postgresql\n    \"\"\"\n    query = 'your PostgreSQL query'  # Specify your SQL query here\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        return loader.load(query)\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_overall_table.py", "language": "python", "type": "data_loader", "uuid": "load_overall_table"}, "data_loaders/idealista_loader.py:data_loader:python:idealista loader": {"content": "import requests\nimport pandas as pd\nimport os\nfrom shapely.geometry import Point, Polygon\nfrom dotenv import load_dotenv\nimport time\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n# Load environment variables from the .env file\nload_dotenv()\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Fetch all data from the Idealista API for Portugal by iterating over grid points in batches.\n    Filter the listings to include only those inside Portugal's boundaries.\n    \"\"\"\n    # Step 1: Get OAuth Token\n    def get_access_token():\n        url = \"https://api.idealista.com/oauth/token\"\n        headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n        data = {\n            \"grant_type\": \"client_credentials\",\n            \"scope\": \"read\",\n            \"client_id\": os.getenv('CLIENT_ID'),\n            \"client_secret\": os.getenv('CLIENT_SECRET'),\n        }\n        response = requests.post(url, headers=headers, data=data)\n        response.raise_for_status()\n        return response.json()[\"access_token\"]\n\n    # Step 2: Make the API call for a specific center point and page\n    def fetch_real_estate_data(token, center, page):\n        url = \"https://api.idealista.com/3.5/pt/search\"\n        headers = {\"Authorization\": f\"Bearer {token}\"}\n        data = {\n            \"country\": \"pt\",\n            \"operation\": \"sale\",\n            \"propertyType\": \"homes\",\n            \"center\": center,\n            \"distance\": \"10000\",  # Larger radius to reduce grid density\n            \"maxItems\": 50,       # Fetch 50 items per page\n            \"numPage\": page,      # Specify the page number\n        }\n\n        retries = 5\n        for attempt in range(retries):\n            try:\n                response = requests.post(url, headers=headers, data=data)\n                response.raise_for_status()\n                return response.json()\n            except requests.exceptions.HTTPError as e:\n                if response.status_code == 429:  # Handle rate limit\n                    print(f\"Rate limit hit. Retrying in {2 ** attempt} seconds...\")\n                    time.sleep(2 ** attempt)  # Exponential backoff\n                else:\n                    raise e\n        raise Exception(\"Failed to fetch data after multiple retries.\")\n\n    # Define Portugal's boundaries using a polygon\n    portugal_polygon = Polygon([\n        (-9.5268, 36.8385),  # Southwest corner\n        (-6.189, 36.9966),   # Southeast corner\n        (-6.3855, 42.1543),  # Northeast corner\n        (-9.5143, 41.9787),  # Northwest corner\n        (-9.5268, 36.8385),  # Closing the polygon\n    ])\n\n    # Helper function to check if a point is within Portugal\n    def is_within_portugal(lat, lon):\n        try:\n            point = Point(lon, lat)  # Shapely expects (longitude, latitude)\n            return portugal_polygon.contains(point)\n        except ValueError:\n            return False\n\n    # Generate a grid of center points across Portugal (smaller batch)\n    center_points = [\n        \"38.7169,-9.1390\",  # Lisbon\n        \"41.1579,-8.6291\",  # Porto\n        \"37.0179,-7.9304\",  # Faro\n        \"39.8222,-7.4909\",  # Castelo Branco\n        \"40.5373,-7.2670\",  # Guarda\n    ]\n\n    all_data = pd.DataFrame()\n\n    try:\n        # Authenticate\n        token = get_access_token()\n\n        # Process the center points in batches\n        batch_size = 2  # Adjust the batch size as needed\n        for i in range(0, len(center_points), batch_size):\n            batch = center_points[i:i + batch_size]\n            print(f\"Processing batch: {batch}\")\n            for center in batch:\n                print(f\"Fetching data for center: {center}\")\n                page = 1\n                while True:\n                    api_response = fetch_real_estate_data(token, center, page)\n                    element_list = api_response.get(\"elementList\", [])\n                    if not element_list:  # If no more data, break the loop\n                        break\n\n                    df = pd.DataFrame(element_list)\n                    if 'latitude' in df.columns and 'longitude' in df.columns:\n                        # Filter listings within Portugal\n                        df = df[df.apply(lambda x: is_within_portugal(x['latitude'], x['longitude']), axis=1)]\n                        all_data = pd.concat([all_data, df], ignore_index=True)\n\n                    print(f\"Page {page} fetched for center: {center}\")\n                    page += 1\n                    time.sleep(1)  # Small delay between pages\n\n            # Delay between batches to avoid hitting the rate limit\n            print(\"Waiting before processing the next batch...\")\n            time.sleep(10)  # Add a longer delay between batches\n\n        # Print the total number of rows\n        total_rows = all_data.shape[0]\n        print(f\"Total number of rows: {total_rows}\")\n\n        print(\"Filtered data inside Portugal:\")\n        print(all_data.head())\n        return all_data\n    except Exception as e:\n        print(f\"Error during API call: {e}\")\n        return pd.DataFrame()\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Test the output of the data loader block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n    assert not output.empty, 'The output DataFrame is empty'\n    assert 'propertyCode' in output.columns, 'PropertyCode is missing in the output'\n    assert 'latitude' in output.columns and 'longitude' in output.columns, 'Latitude and Longitude columns are missing'\n", "file_path": "data_loaders/idealista_loader.py", "language": "python", "type": "data_loader", "uuid": "idealista_loader"}, "data_loaders/load_real_estate_data_table.sql:data_loader:sql:load real estate data table": {"content": "-- Docs: https://docs.mage.ai/guides/sql-blocks\n", "file_path": "data_loaders/load_real_estate_data_table.sql", "language": "sql", "type": "data_loader", "uuid": "load_real_estate_data_table"}, "data_loaders/loads_real_estate_table.py:data_loader:python:loads real estate table": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom os import path\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_postgres(*args, **kwargs):\n    \"\"\"\n    Load the overall PostgreSQL table into a DataFrame.\n    Specify your configuration settings in 'io_config.yaml'.\n    \"\"\"\n    # SQL query to fetch data from the overall table\n    query = \"\"\"\n    SELECT *\n    FROM public.real_estate_data  -- Replace with your actual table name and schema\n    \"\"\"\n    \n    # Path to the configuration file\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    # Connect to PostgreSQL and execute the query\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        print(f\"Loading data from PostgreSQL table 'public.real_estate_data'...\")\n        return loader.load(query)\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Test that the output DataFrame is not None or empty.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n    assert not output.empty, 'The output DataFrame is empty'\n", "file_path": "data_loaders/loads_real_estate_table.py", "language": "python", "type": "data_loader", "uuid": "loads_real_estate_table"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "data_loaders/gsc_test.py:data_loader:python:gsc test": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom os import path\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_from_google_cloud_storage(*args, **kwargs):\n    \"\"\"\n    Template for loading data from a Google Cloud Storage bucket.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#googlecloudstorage\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    bucket_name = 'your_bucket_name'\n    object_key = 'your_object_key'\n\n    return GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).load(\n        bucket_name,\n        object_key,\n    )\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/gsc_test.py", "language": "python", "type": "data_loader", "uuid": "gsc_test"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "transformers/cleans_dataframe.py:transformer:python:cleans dataframe": {"content": "from mage_ai.data_cleaner.transformer_actions.base import BaseAction\nfrom mage_ai.data_cleaner.transformer_actions.constants import ActionType, Axis\nfrom mage_ai.data_cleaner.transformer_actions.utils import build_transformer_action\nfrom pandas import DataFrame\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef execute_transformer_action(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Clean column names, add a 'unique_id' column, and move it to the first position.\n    \"\"\"\n    # Step 1: Clean column names\n    action = build_transformer_action(\n        df,\n        action_type=ActionType.CLEAN_COLUMN_NAME,\n        arguments=df.columns,\n        axis=Axis.COLUMN,\n    )\n    df = BaseAction(action).execute(df)\n\n    # Step 2: Rename '_type' to 'type' if it exists\n    if '_type' in df.columns:\n        print(\"Renaming '_type' to 'type'...\")\n        df.rename(columns={'_type': 'type'}, inplace=True)\n\n    # Step 3: Add 'unique_id' column if not already present\n    if 'unique_id' not in df.columns:\n        print(\"Adding 'unique_id' column...\")\n        def generate_unique_id(row):\n            # Numbers: Use the full value\n            total_area = str(row.get('totalarea', ''))\n\n            # Letters: Use the first letter of each word\n            floor = ''.join([word[0] for word in str(row.get('floor', '')).split()])\n            district = ''.join([word[0] for word in str(row.get('district', '')).split()])\n            city = ''.join([word[0] for word in str(row.get('city', '')).split()])\n            town = ''.join([word[0] for word in str(row.get('town', '')).split()])\n            property_type = ''.join([word[0] for word in str(row.get('type', '')).split()])\n\n            # Combine all parts into a unique_id without hyphens\n            return f\"{floor}{total_area}{district}{city}{town}{property_type}\"\n\n        df['unique_id'] = df.apply(generate_unique_id, axis=1)\n\n    # Step 4: Move 'unique_id' to the first column\n    columns = ['unique_id'] + [col for col in df.columns if col != 'unique_id']\n    df = df[columns]\n\n    # Step 5: Return the cleaned DataFrame\n    return df\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n    assert output.columns[0] == 'unique_id', \"'unique_id' column is not the first column in the output.\"\n    assert 'type' in output.columns, \"'type' column is missing from the output.\"\n    assert '_type' not in output.columns, \"'_type' column still exists in the output.\"\n", "file_path": "transformers/cleans_dataframe.py", "language": "python", "type": "transformer", "uuid": "cleans_dataframe"}, "transformers/remove_null_price.py:transformer:python:remove null price": {"content": "from mage_ai.data_cleaner.transformer_actions.base import BaseAction\nfrom mage_ai.data_cleaner.transformer_actions.constants import ActionType, Axis\nfrom mage_ai.data_cleaner.transformer_actions.utils import build_transformer_action\nfrom pandas import DataFrame\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef execute_transformer_action(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Remove rows where the price is null or exceeds 250 million EUR.\n    \"\"\"\n    # Define the maximum price limit\n    max_price = 250_000_000\n\n    df = df[\n            df['price'].notnull() & \n            (df['price'] <= max_price) & \n            (df['district'] != 'Z - Fora de Portugal')\n        ].reset_index(drop=True)\n\n    return df\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Test to ensure no rows have a null price or a price over 250 million EUR.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n    assert not output['price'].isnull().any(), 'There are rows with null prices in the output.'\n    assert (output['price'] <= 250_000_000).all(), 'There are rows with a price exceeding 250 million EUR.'\n", "file_path": "transformers/remove_null_price.py", "language": "python", "type": "transformer", "uuid": "remove_null_price"}, "transformers/takes_ducplicate_rows.sql:transformer:sql:takes ducplicate rows": {"content": "-- Docs: https://docs.mage.ai/guides/sql-blocks\n", "file_path": "transformers/takes_ducplicate_rows.sql", "language": "sql", "type": "transformer", "uuid": "takes_ducplicate_rows"}, "transformers/deletes_duplications.py:transformer:python:deletes duplications": {"content": "", "file_path": "transformers/deletes_duplications.py", "language": "python", "type": "transformer", "uuid": "deletes_duplications"}, "transformers/remove_rows_withou_info.py:transformer:python:remove rows withou info": {"content": "from mage_ai.data_cleaner.transformer_actions.base import BaseAction\nfrom mage_ai.data_cleaner.transformer_actions.constants import ActionType, Axis\nfrom mage_ai.data_cleaner.transformer_actions.utils import build_transformer_action\nfrom pandas import DataFrame\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef execute_transformer_action(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Execute Transformer Action: Remove empty rows and rows where 'province' does not exist.\n    \"\"\"\n\n    # Remove rows where 'province' column does not exist or is empty\n    if 'province' in df.columns:\n        df = df[df['province'].notna() & (df['province'] != '')]\n\n    return df\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n    assert not output.empty, 'The output DataFrame is empty'\n    assert 'province' in output.columns, 'Expected column \"province\" not found in output'\n    assert output['province'].notna().all(), 'There are rows with missing \"province\" information'", "file_path": "transformers/remove_rows_withou_info.py", "language": "python", "type": "transformer", "uuid": "remove_rows_withou_info"}, "transformers/drops_duplicate_rows.py:transformer:python:drops duplicate rows": {"content": "from mage_ai.data_cleaner.transformer_actions.base import BaseAction\nfrom mage_ai.data_cleaner.transformer_actions.constants import ActionType, Axis\nfrom mage_ai.data_cleaner.transformer_actions.utils import build_transformer_action\nfrom pandas import DataFrame\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef execute_transformer_action(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Execute Transformer Action: ActionType.DROP_DUPLICATE\n\n    Docs: https://docs.mage.ai/guides/transformer-blocks#drop-duplicates\n    \"\"\"\n    action = build_transformer_action(\n        df,\n        action_type=ActionType.DROP_DUPLICATE,\n        arguments=df.columns,  # Specify column names to use when comparing duplicates\n        axis=Axis.ROW,\n        options={'keep': 'first'},  # Specify whether to keep 'first' or 'last' duplicate\n    )\n\n    return BaseAction(action).execute(df)\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/drops_duplicate_rows.py", "language": "python", "type": "transformer", "uuid": "drops_duplicate_rows"}, "transformers/tablecomparison.py:transformer:python:tablecomparison": {"content": "from pandas import DataFrame\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef remove_duplicates(weekly_df: DataFrame, overall_df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Compares weekly_df with overall_df and removes rows in weekly_df\n    that have duplicate 'unique_id' values in overall_df.\n    \"\"\"\n    # Ensure the column for comparison exists in both DataFrames\n    column_to_compare = 'unique_id'\n    if column_to_compare not in weekly_df.columns:\n        raise ValueError(f\"Column '{column_to_compare}' is missing from weekly_df.\")\n    if column_to_compare not in overall_df.columns:\n        raise ValueError(f\"Column '{column_to_compare}' is missing from overall_df.\")\n\n    # Drop rows in weekly_df where 'unique_id' exists in overall_df\n    print(f\"Removing duplicates based on '{column_to_compare}'...\")\n    filtered_weekly_df = weekly_df[~weekly_df[column_to_compare].isin(overall_df[column_to_compare])]\n\n    # Log the number of rows before and after filtering\n    print(f\"Rows in weekly_df before filtering: {len(weekly_df)}\")\n    print(f\"Rows in weekly_df after filtering: {len(filtered_weekly_df)}\")\n\n    return filtered_weekly_df\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Test to ensure duplicates are removed correctly.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n    assert isinstance(output, DataFrame), 'The output is not a DataFrame'\n", "file_path": "transformers/tablecomparison.py", "language": "python", "type": "transformer", "uuid": "tablecomparison"}, "pipelines/test_autentications/metadata.yaml:pipeline:yaml:test autentications/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - gcs_tester\n  - postgres_tester\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: api_test_call\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: api_test_call\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: GCS tester\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - api_test_call\n  uuid: gcs_tester\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: postgres_tester\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - api_test_call\n  uuid: postgres_tester\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-01-15 10:04:02.902487+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: test_autentications\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: test_autentications\nvariables_dir: /home/src/mage_data/de-zoomcamp-project\nwidgets: []\n", "file_path": "pipelines/test_autentications/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "test_autentications/metadata"}, "pipelines/real_estate/metadata.yaml:pipeline:yaml:real estate/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_loaders/kaggle_loader.py\n    file_source:\n      path: data_loaders/kaggle_loader.py\n  downstream_blocks:\n  - cleans_dataframe\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: kaggle_loader\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: kaggle_loader\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - remove_null_price\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: cleans dataframe\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - kaggle_loader\n  uuid: cleans_dataframe\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - data_exporter_weely_postgres\n  - tablecomparison\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: drops_duplicate_rows\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - remove_null_price\n  uuid: drops_duplicate_rows\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_exporters/data_exporter_weely_postgres.py\n    file_source:\n      path: data_exporters/data_exporter_weely_postgres.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: data_exporter_weely_postgres\n  retry_config: null\n  status: failed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - drops_duplicate_rows\n  uuid: data_exporter_weely_postgres\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - tablecomparison\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: loads_real_estate_table\n  retry_config: null\n  status: failed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: loads_real_estate_table\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - export_real_estate_data\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: tablecomparison\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - drops_duplicate_rows\n  - loads_real_estate_table\n  uuid: tablecomparison\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: export_real_estate_data\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - tablecomparison\n  uuid: export_real_estate_data\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - drops_duplicate_rows\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: remove null price\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - cleans_dataframe\n  uuid: remove_null_price\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-01-16 07:53:09.712262+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: real_estate\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: real_estate\nvariables_dir: /home/src/mage_data/de-zoomcamp-project\nwidgets: []\n", "file_path": "pipelines/real_estate/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "real_estate/metadata"}, "pipelines/real_estate/__init__.py:pipeline:python:real estate/  init  ": {"content": "", "file_path": "pipelines/real_estate/__init__.py", "language": "python", "type": "pipeline", "uuid": "real_estate/__init__"}, "pipelines/de_zoomcamp_project/metadata.yaml:pipeline:yaml:de zoomcamp project/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_loaders/idealista_loader.py\n    file_source:\n      path: data_loaders/idealista_loader.py\n  downstream_blocks:\n  - remove_rows_withou_info\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: idealista_loader\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: idealista_loader\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - export_sql\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: Remove rows withou info\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - idealista_loader\n  uuid: remove_rows_withou_info\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: export_sql\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - remove_rows_withou_info\n  uuid: export_sql\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-01-15 12:11:43.997748+00:00'\ndata_integration: null\ndescription: The pipeline fetches real estate data from the Idealista API and processes\n  it in parallel to store in two destination\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: de-zoomcamp-project\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags:\n- de-zoomcamp\ntype: python\nuuid: de_zoomcamp_project\nvariables_dir: /home/src/mage_data/de-zoomcamp-project\nwidgets: []\n", "file_path": "pipelines/de_zoomcamp_project/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "de_zoomcamp_project/metadata"}, "pipelines/de_zoomcamp_project/__init__.py:pipeline:python:de zoomcamp project/  init  ": {"content": "", "file_path": "pipelines/de_zoomcamp_project/__init__.py", "language": "python", "type": "pipeline", "uuid": "de_zoomcamp_project/__init__"}}, "custom_block_template": {}, "mage_template": {"data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}