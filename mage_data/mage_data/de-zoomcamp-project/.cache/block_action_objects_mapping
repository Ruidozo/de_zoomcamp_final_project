{"block_file": {"conditionals/spirited_haze.py:conditional:python:spirited haze": {"content": "if 'condition' not in globals():\n    from mage_ai.data_preparation.decorators import condition\nfrom mage_ai.data_preparation.models.block import Block\n\n@condition\ndef evaluate_condition(*args, **kwargs) -> bool:\n    \"\"\"\n    Conditional check to evaluate if 'export_real_estate_data' has been executed.\n    \"\"\"\n    # Access the pipeline instance\n    pipeline = kwargs.get('pipeline')\n    \n    # Get the block by its name\n    export_block = pipeline.get_block('export_real_estate_data')\n    if export_block is None:\n        raise ValueError(\"Block 'export_real_estate_data' not found in the pipeline.\")\n\n    # Check the status of the block\n    if export_block.status == Block.Status.COMPLETED:\n        print(\"Block 'export_real_estate_data' has been executed. Proceeding...\")\n        return True\n    else:\n        print(\"Block 'export_real_estate_data' has not been executed. Skipping downstream blocks.\")\n        return False\n", "file_path": "conditionals/spirited_haze.py", "language": "python", "type": "conditional", "uuid": "spirited_haze"}, "conditionals/trigger.py:conditional:python:trigger": {"content": "if 'condition' not in globals():\n    from mage_ai.data_preparation.decorators import condition\n\n\n@condition\ndef evaluate_condition(*args, **kwargs) -> bool:\n    \"\"\"\n    Check if the 'export_real_estate_data' block has been executed successfully.\n    \"\"\"\n    # Retrieve the pipeline context from kwargs\n    pipeline = kwargs.get('pipeline')\n    if pipeline is None:\n        print(\"Pipeline context is not available.\")\n        return False\n\n    # Get the specific block by name\n    export_block = pipeline.get_block('export_real_estate_data')\n    if export_block is None:\n        print(\"Block 'export_real_estate_data' not found.\")\n        return False\n\n    # Check the execution status of the block\n    if export_block.status == 'completed':  # Replace with the correct status check if different\n        print(\"Block 'export_real_estate_data' has been executed. Proceeding...\")\n        return True\n    else:\n        print(\"Block 'export_real_estate_data' has not been executed. Skipping downstream blocks.\")\n        return False\n", "file_path": "conditionals/trigger.py", "language": "python", "type": "conditional", "uuid": "trigger"}, "conditionals/golden_ronin.py:conditional:python:golden ronin": {"content": "if 'condition' not in globals():\n    from mage_ai.data_preparation.decorators import condition\nfrom mage_ai.data_preparation.repo_manager import get_repo_path\nfrom mage_ai.data_preparation.variable_manager import VariableManager\n\n\n@condition\ndef evaluate_condition(*args, **kwargs) -> bool:\n    \"\"\"\n    Evaluate whether the DBT block should run based on the presence of new rows.\n    \"\"\"\n    # Access the global variable to check if there are new rows\n    variable_manager = VariableManager(get_repo_path())\n    has_new_rows = variable_manager.get_variable('default', 'has_new_rows')\n\n    # Return True if there are new rows, False otherwise\n    return has_new_rows\n", "file_path": "conditionals/golden_ronin.py", "language": "python", "type": "conditional", "uuid": "golden_ronin"}, "conditionals/charmed_music.py:conditional:python:charmed music": {"content": "if 'condition' not in globals():\n    from mage_ai.data_preparation.decorators import condition\n\n\n@condition\ndef evaluate_condition(*args, **kwargs) -> bool:\n    return True\n", "file_path": "conditionals/charmed_music.py", "language": "python", "type": "conditional", "uuid": "charmed_music"}, "custom/webhook_slack.py:custom:python:webhook slack": {"content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\nimport requests\nimport os\nfrom dotenv import load_dotenv\n\n# Load .env file\nload_dotenv()\n\ndef notify_slack(pipeline_name, status, db_data=None):\n    \"\"\"\n    Sends a notification to Slack with the pipeline status and the number of rows exported.\n    \"\"\"\n    # Access the webhook URL from environment variables\n    webhook_url = os.getenv('SLACK_WEBHOOK_URL')\n\n    if not webhook_url:\n        raise ValueError(\"Slack webhook URL is not set in the environment variables.\")\n\n    # Prepare the message\n    if db_data and isinstance(db_data, int):\n        message_text = (\n            f\"Pipeline {pipeline_name} has completed with status: {status}.\\n\"\n            f\"Number of rows exported to the database: {db_data}\"\n        )\n    else:\n        message_text = f\"Pipeline {pipeline_name} has completed with status: {status}.\\nNo data was exported.\"\n\n    message = {\"text\": message_text}\n\n    # Send the Slack message\n    response = requests.post(webhook_url, json=message)\n\n    # Debug: Print the response\n    print(f\"Response status code: {response.status_code}\")\n    print(f\"Response text: {response.text}\")\n\n\n@custom\ndef transform_custom(*args, **kwargs):\n    \"\"\"\n    Sends a Slack notification with the number of rows exported.\n    \"\"\"\n    # Retrieve the DataFrame from the upstream block\n    df_data = args[0] if args else None\n\n    # Calculate the number of rows exported\n    num_rows_exported = len(df_data) if df_data is not None else 0\n\n    # Debug: Print the row count\n    print(f\"Number of rows exported: {num_rows_exported}\")\n\n    # Notify Slack\n    notify_slack(\n        pipeline_name=\"data_real_estate\",\n        status=\"Success\",\n        db_data=num_rows_exported\n    )\n\n    return num_rows_exported\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "custom/webhook_slack.py", "language": "python", "type": "custom", "uuid": "webhook_slack"}, "custom/data_evaluator.py:custom:python:data evaluator": {"content": "from mage_ai.io.postgres import Postgres\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.settings.repo import get_repo_path\nfrom os import path\nimport pandas as pd\nimport time\n\nif 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@custom\ndef transform_custom(*args, **kwargs):\n    \"\"\"\n    Calculate detailed statistics for all columns in a PostgreSQL table.\n    Return only the evaluation data for export, with a 1-minute delay before execution.\n    \"\"\"\n    # Load PostgreSQL configuration\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    # Specify the table name\n    table_name = 'public.real_estate_data_weekly'\n\n    # Add a delay \n    delay_seconds = 30\n    print(f\"Delaying execution by {delay_seconds} seconds...\")\n    time.sleep(delay_seconds)\n    print(\"Resuming execution after delay.\")\n\n    def summary(df):\n        \"\"\"\n        Calculate detailed statistics for a DataFrame and include column names.\n        \"\"\"\n        print(f\"Data shape: {df.shape}\")\n\n        # Start building the summary DataFrame\n        summary_data = {\n            \"Column Name\": df.columns,  # Add column names explicitly\n            \"Data Type\": df.dtypes.astype(str),  # Convert dtypes to string\n            \"Missing percent\": (df.isna().sum() / len(df)) if len(df) > 0 else 0,\n            \"Missing quant\": df.isna().sum(),\n            \"Dups\": [df.duplicated().sum()] * len(df.columns),  # Apply duplicated count to all rows\n            \"Uniques\": df.nunique().values,\n            \"Count\": df.count().values,\n        }\n\n        # Calculate numeric statistics (only for numeric columns)\n        numeric_columns = df.select_dtypes(include=[\"number\"])\n        if not numeric_columns.empty:\n            numeric_stats = numeric_columns.describe().transpose()\n            summary_data[\"Min\"] = numeric_stats[\"min\"].reindex(df.columns, fill_value=None)\n            summary_data[\"Max\"] = numeric_stats[\"max\"].reindex(df.columns, fill_value=None)\n            summary_data[\"Average\"] = numeric_stats[\"mean\"].reindex(df.columns, fill_value=None)\n            summary_data[\"Standard Deviation\"] = numeric_stats[\"std\"].reindex(df.columns, fill_value=None)\n        else:\n            # Add empty placeholders for numeric stats if there are no numeric columns\n            summary_data[\"Min\"] = [None] * len(df.columns)\n            summary_data[\"Max\"] = [None] * len(df.columns)\n            summary_data[\"Average\"] = [None] * len(df.columns)\n            summary_data[\"Standard Deviation\"] = [None] * len(df.columns)\n\n        # Create the summary DataFrame\n        summ = pd.DataFrame(summary_data)\n\n        # Cast columns to match PostgreSQL schema\n        summ[\"Missing percent\"] = summ[\"Missing percent\"].astype(float)  # Cast to float\n        summ[\"Missing quant\"] = summ[\"Missing quant\"].astype(int)  # Cast to integer\n\n        # Replace NaN values with appropriate defaults\n        for col in summ.columns:\n            if summ[col].isna().any():\n                print(f\"Column '{col}' contains NaN values. Filling with appropriate value.\")\n                if summ[col].dtype in ['float64', 'int64']:  # Numeric columns\n                    summ[col] = summ[col].fillna(0)  # Replace NaN with 0 for numeric\n                else:  # Object columns\n                    summ[col] = summ[col].fillna(\"\")  # Replace NaN with empty string for objects\n\n        # Drop rows where all numeric values are 0\n        numeric_cols = summ.select_dtypes(include=[\"number\"]).columns\n        summ = summ[~(summ[numeric_cols].sum(axis=1) == 0)]\n\n        return summ\n\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        # Fetch all data from the table\n        query_fetch_data = f\"SELECT * FROM {table_name};\"\n        df = loader.load(query_fetch_data)\n\n        # Debugging: Print DataFrame columns\n        print(\"Original DataFrame Columns:\", df.columns)\n\n        # Align DataFrame columns to match PostgreSQL table\n        expected_columns = [\n            \"unique_id\", \"price\", \"district\", \"city\", \"town\", \"_type\",\n            \"energy_certificate\", \"gross_area\", \"total_area\", \"parking\",\n            \"has_parking\", \"_floor\", \"construction_year\", \"energy_efficiency_level\",\n            \"publish_date\", \"garage\", \"elevator\", \"electric_cars_charging\",\n            \"total_rooms\", \"number_of_bedrooms\", \"number_of_w_c\",\n            \"conservation_status\", \"living_area\", \"lot_size\", \"built_area\",\n            \"number_of_bathrooms\"\n        ]\n        df = df[expected_columns]\n\n        print(\"Post-alignment DataFrame Columns:\", df.columns)\n        print(f\"Data fetched: {df.shape}\")\n\n        # Calculate summary statistics\n        stats = summary(df)\n\n        # Return only the evaluation data (summary statistics)\n        print(f\"Data_stats fetched: {stats.shape}\")\n        return stats\n", "file_path": "custom/data_evaluator.py", "language": "python", "type": "custom", "uuid": "data_evaluator"}, "data_exporters/gcs_tester.py:data_exporter:python:gcs tester": {"content": "from mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.settings.repo import get_repo_path\nfrom pandas import DataFrame\nfrom os import path\n\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_google_cloud_storage(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Export data to a Google Cloud Storage bucket, creating the bucket if it does not exist.\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    bucket_name = 'mage_test_setup'\n    object_key = 'real_estate_data.csv'\n\n    gcs = GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile))\n\n    # Ensure the bucket exists\n    def ensure_bucket_exists(bucket_name):\n        client = gcs.client\n        try:\n            bucket = client.get_bucket(bucket_name)\n            print(f\"Bucket '{bucket_name}' already exists.\")\n        except Exception:\n            bucket = client.bucket(bucket_name)\n            bucket = client.create_bucket(bucket, location=\"europe-west2\")\n            print(f\"Bucket '{bucket_name}' created in location 'europe-west2'.\")\n\n    # Ensure the bucket exists before exporting\n    ensure_bucket_exists(bucket_name)\n\n    # Export the DataFrame to the GCS bucket\n    gcs.export(\n        df,\n        bucket_name,\n        object_key,\n    )\n\n    print(f\"Data exported to GCS bucket '{bucket_name}' at '{object_key}'.\")\n\n    # List all buckets in the project\n    print(\"\\nListing all GCS buckets:\")\n    for bucket in gcs.client.list_buckets():\n        print(f\"- {bucket.name}\")\n", "file_path": "data_exporters/gcs_tester.py", "language": "python", "type": "data_exporter", "uuid": "gcs_tester"}, "data_exporters/export_real_estate_data.py:data_exporter:python:export real estate data": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_postgres(df: DataFrame, **kwargs) -> DataFrame:\n    \"\"\"\n    Append the filtered data to the real_estate_data table in PostgreSQL and return the DataFrame.\n    \"\"\"\n    schema_name = 'public'\n    table_name = 'real_estate_data'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    if df is None or df.empty:\n        print(\"No new data to append. Exiting block.\")\n        return None\n\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        loader.export(\n            df,\n            schema_name,\n            table_name,\n            index=False,\n            if_exists='append',\n        )\n    print(f\"Data appended successfully to '{schema_name}.{table_name}'.\")\n    return df  # Return the DataFrame\n", "file_path": "data_exporters/export_real_estate_data.py", "language": "python", "type": "data_exporter", "uuid": "export_real_estate_data"}, "data_exporters/data_exporter_stats.py:data_exporter:python:data exporter stats": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom os import path\nimport pandas as pd\nimport numpy as np\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_postgres(stats: DataFrame, **kwargs) -> DataFrame:\n    \"\"\"\n    Export the evaluation DataFrame (summary statistics) to a PostgreSQL table\n    and return the DataFrame for downstream use.\n    \"\"\"\n    schema_name = 'public'  # Specify the schema\n    table_name = 'stats'  # Specify the table name\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    # Debug input type and content\n    print(f\"Received input type: {type(stats)}\")\n    if not isinstance(stats, pd.DataFrame):\n        raise ValueError(f\"Expected a DataFrame, but got {type(stats)} instead.\")\n\n    # Debug DataFrame structure\n    print(f\"DataFrame contains {len(stats)} rows and {len(stats.columns)} columns.\")\n    print(f\"DataFrame preview:\\n{stats}\")\n\n    # Ensure data types align with PostgreSQL schema\n    stats[\"Missing percent\"] = stats[\"Missing percent\"].astype(float)  # Ensure float\n    stats[\"Missing quant\"] = stats[\"Missing quant\"].astype(int)  # Ensure integer\n\n    # Replace NaN and invalid values\n    stats = stats.replace({np.nan: None, \"NaN\": None, pd.NA: None})\n\n    # Export DataFrame to PostgreSQL\n    try:\n        with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n            loader.export(\n                stats,\n                schema_name=schema_name,\n                table_name=table_name,\n                index=False,  # Exclude the DataFrame index\n                if_exists='replace',  # Replace the table if it already exists\n            )\n        print(f\"Data exported successfully to '{schema_name}.{table_name}'.\")\n    except Exception as e:\n        print(f\"Error during export: {e}\")\n        raise\n\n    # Return the DataFrame for downstream processing\n    return stats\n", "file_path": "data_exporters/data_exporter_stats.py", "language": "python", "type": "data_exporter", "uuid": "data_exporter_stats"}, "data_exporters/postgres_tester.py:data_exporter:python:postgres tester": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_postgres(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Export Kaggle dataset to a PostgreSQL database with proper table recreation and schema alignment.\n    \"\"\"\n    import pandas as pd\n\n    # Validate DataFrame\n    if df is None or df.empty:\n        raise ValueError(\"The DataFrame is empty or undefined. Cannot export to PostgreSQL.\")\n\n    print(f\"Original DataFrame columns: {df.columns}\")\n\n    # Clean column names to match PostgreSQL requirements\n    df.columns = (\n        df.columns.str.strip()                 # Remove leading/trailing spaces\n        .str.lower()                           # Convert to lowercase\n        .str.replace(' ', '_')                # Replace spaces with underscores\n        .str.replace(r'[^a-zA-Z0-9_]', '')    # Remove invalid characters\n    )\n    print(f\"Cleaned DataFrame columns: {df.columns}\")\n\n    # Set schema and table name\n    schema_name = 'public'  # Default PostgreSQL schema\n    table_name = 'stats'\n\n    # Load PostgreSQL configuration\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        # Drop the table if it exists (explicitly)\n        print(f\"Dropping table '{schema_name}.{table_name}' if it exists...\")\n        loader.execute(f\"DROP TABLE IF EXISTS {schema_name}.{table_name} CASCADE;\")\n\n        # Export the DataFrame to PostgreSQL\n        loader.export(\n            df,\n            schema_name=schema_name,\n            table_name=table_name,\n            index=False,        # Do not export the index\n            if_exists='replace' # Replace the table if it already exists\n        )\n    print(f\"Data exported to PostgreSQL table '{schema_name}.{table_name}'.\")\n", "file_path": "data_exporters/postgres_tester.py", "language": "python", "type": "data_exporter", "uuid": "postgres_tester"}, "data_exporters/data_exporter_weely_postgres.py:data_exporter:python:data exporter weely postgres": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_postgres(df: DataFrame, *args, **kwargs) -> None:\n    \"\"\"\n    Export the cleaned DataFrame from the transformer block to a PostgreSQL database.\n    Specify your configuration settings in 'io_config.yaml'.\n    \"\"\"\n    # Define the schema and table names\n    schema_name = 'public'  # Adjust this if you have a custom schema\n    table_name = 'real_estate_data_weekly'  # Name of the table for the exported data\n\n    # Path to the database configuration file\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    # Validate the DataFrame\n    if df is None or df.empty:\n        raise ValueError(\"The DataFrame is empty or undefined. Cannot export to PostgreSQL.\")\n\n    print(f\"Exporting DataFrame to PostgreSQL table '{schema_name}.{table_name}'...\")\n    print(f\"DataFrame columns: {df.columns}\")\n\n    # Export the DataFrame to PostgreSQL\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        loader.export(\n            df,\n            schema_name=schema_name,\n            table_name=table_name,\n            index=False,  # Do not include the DataFrame index in the exported table\n            if_exists='replace',  # Replace the table if it already exists\n        )\n    print(f\"Data exported successfully to '{schema_name}.{table_name}'.\")\n", "file_path": "data_exporters/data_exporter_weely_postgres.py", "language": "python", "type": "data_exporter", "uuid": "data_exporter_weely_postgres"}, "data_loaders/loads_real_estate_table.py:data_loader:python:loads real estate table": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom os import path\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_postgres(*args, **kwargs):\n    \"\"\"\n    Load the overall PostgreSQL table into a DataFrame.\n    Specify your configuration settings in 'io_config.yaml'.\n    \"\"\"\n    # SQL query to fetch data from the overall table\n    query = \"\"\"\n    SELECT *\n    FROM public.real_estate_data  -- Replace with your actual table name and schema\n    \"\"\"\n    \n    # Path to the configuration file\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    # Connect to PostgreSQL and execute the query\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        print(f\"Loading data from PostgreSQL table 'public.real_estate_data'...\")\n        return loader.load(query)\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Test that the output DataFrame is not None or empty.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n    assert not output.empty, 'The output DataFrame is empty'\n", "file_path": "data_loaders/loads_real_estate_table.py", "language": "python", "type": "data_loader", "uuid": "loads_real_estate_table"}, "data_loaders/kaggle_loader.py:data_loader:python:kaggle loader": {"content": "import os\nimport pandas as pd\nimport kagglehub\nimport gc  # Import garbage collection module\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n@data_loader\ndef load_data_from_kaggle(*args, **kwargs):\n    \"\"\"\n    Load data from Kaggle using KaggleHub.\n    Free memory before starting the pipeline.\n    \"\"\"\n    try:\n        # Free up memory before loading data\n        gc.collect()\n        print(\"Memory freed before loading data.\")\n\n        # Download the latest version of the dataset\n        path = kagglehub.dataset_download(\"luvathoms/portugal-real-estate-2024\")\n        print(\"Path to dataset files:\", path)\n\n        # Print the contents of the directory for debugging\n        print(\"Downloaded files:\", os.listdir(path))\n\n        # Look for a CSV file in the dataset directory\n        dataset_file = None\n        for file in os.listdir(path):\n            if file.endswith(\".csv\"):\n                dataset_file = os.path.join(path, file)\n                break\n        \n        if not dataset_file:\n            raise FileNotFoundError(f\"No CSV file found in dataset directory: {path}\")\n        \n        # Load the data into a pandas DataFrame\n        df = pd.read_csv(dataset_file)\n\n        # Display dataset summary\n        print(f\"Dataset loaded with {df.shape[0]} rows and {df.shape[1]} columns.\")\n        print(df.head())\n        \n        return df\n    except Exception as e:\n        print(f\"Error during data loading: {e}\")\n        return pd.DataFrame()\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Test the output of the Kaggle data loader block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n    assert not output.empty, 'The output DataFrame is empty'\n", "file_path": "data_loaders/kaggle_loader.py", "language": "python", "type": "data_loader", "uuid": "kaggle_loader"}, "data_loaders/api_test_call.py:data_loader:python:api test call": {"content": "import requests\nimport pandas as pd\nimport os\nfrom dotenv import load_dotenv\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n# Load environment variables from the .env file\nload_dotenv('/home/src/.env')\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Load data from the Idealista API using a POST request.\n    \"\"\"\n    # Step 1: Get OAuth Token\n    def get_access_token():\n        url = \"https://api.idealista.com/oauth/token\"\n        headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n        data = {\n            \"grant_type\": \"client_credentials\",\n            \"scope\": \"read\",\n            \"client_id\": os.getenv('CLIENT_ID'),  # Corrected syntax: Keys must be strings\n            \"client_secret\": os.getenv('CLIENT_SECRET'),  # Corrected syntax\n        }\n        response = requests.post(url, headers=headers, data=data)\n        response.raise_for_status()  # Raise an error for bad responses\n        return response.json()[\"access_token\"]\n\n    # Step 2: Make the API call\n    def fetch_real_estate_data(token):\n        url = \"https://api.idealista.com/3.5/es/search\"\n        headers = {\"Authorization\": f\"Bearer {token}\"}\n        data = {\n            \"center\": \"40.430,-3.702\",  # Coordinates (latitude, longitude)\n            \"propertyType\": \"homes\",   # Type of property\n            \"distance\": \"15000\",       # Radius in meters\n            \"operation\": \"sale\",       # Operation type\n        }\n        response = requests.post(url, headers=headers, data=data)\n        response.raise_for_status()  # Raise an error for bad responses\n        return response.json()\n\n    try:\n        # Authenticate and fetch data\n        token = get_access_token()\n        api_response = fetch_real_estate_data(token)\n\n        # Extract the element list into a DataFrame\n        element_list = api_response.get(\"elementList\", [])\n        df = pd.DataFrame(element_list)\n\n        print(\"Fetched data:\", df.head())\n        return df\n    except Exception as e:\n        print(f\"Error during API call: {e}\")\n        return pd.DataFrame()\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Test the output of the data loader block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n    assert not output.empty, 'The output DataFrame is empty'\n    assert 'propertyCode' in output.columns, 'PropertyCode is missing in the output'\n", "file_path": "data_loaders/api_test_call.py", "language": "python", "type": "data_loader", "uuid": "api_test_call"}, "transformers/tablecomparison.py:transformer:python:tablecomparison": {"content": "from pandas import DataFrame\nfrom mage_ai.data_preparation.repo_manager import get_repo_path\nfrom mage_ai.data_preparation.variable_manager import VariableManager\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef remove_duplicates(weekly_df: DataFrame, overall_df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Compares weekly_df with overall_df and removes rows in weekly_df\n    that have duplicate 'unique_id' values in overall_df.\n    Also flags if there are new rows to process.\n    \"\"\"\n    # Ensure the column for comparison exists in both DataFrames\n    column_to_compare = 'unique_id'\n    if column_to_compare not in weekly_df.columns:\n        raise ValueError(f\"Column '{column_to_compare}' is missing from weekly_df.\")\n    if column_to_compare not in overall_df.columns:\n        raise ValueError(f\"Column '{column_to_compare}' is missing from overall_df.\")\n\n    # Drop rows in weekly_df where 'unique_id' exists in overall_df\n    print(f\"Removing duplicates based on '{column_to_compare}'...\")\n    filtered_weekly_df = weekly_df[~weekly_df[column_to_compare].isin(overall_df[column_to_compare])]\n\n    # Log the number of rows before and after filtering\n    print(f\"Rows in weekly_df before filtering: {len(weekly_df)}\")\n    print(f\"Rows in weekly_df after filtering: {len(filtered_weekly_df)}\")\n\n    # Check if there are new rows and set a global variable\n    has_new_rows = len(filtered_weekly_df) > 0\n\n    # Initialize the VariableManager and add the variable\n    variable_manager = VariableManager(get_repo_path())\n    variable_manager.add_variable('default', 'has_new_rows', 'has_new_rows', has_new_rows)\n\n    if has_new_rows:\n        print(\"New rows found. Setting variable 'has_new_rows' to True.\")\n    else:\n        print(\"No new rows found. Setting variable 'has_new_rows' to False.\")\n\n    return filtered_weekly_df\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Test to ensure duplicates are removed correctly.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n    assert isinstance(output, DataFrame), 'The output is not a DataFrame'\n", "file_path": "transformers/tablecomparison.py", "language": "python", "type": "transformer", "uuid": "tablecomparison"}, "transformers/drops_duplicate_rows.py:transformer:python:drops duplicate rows": {"content": "from mage_ai.data_cleaner.transformer_actions.base import BaseAction\nfrom mage_ai.data_cleaner.transformer_actions.constants import ActionType, Axis\nfrom mage_ai.data_cleaner.transformer_actions.utils import build_transformer_action\nfrom pandas import DataFrame\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef execute_transformer_action(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Execute Transformer Action: ActionType.DROP_DUPLICATE\n\n    Docs: https://docs.mage.ai/guides/transformer-blocks#drop-duplicates\n    \"\"\"\n    action = build_transformer_action(\n        df,\n        action_type=ActionType.DROP_DUPLICATE,\n        arguments=df.columns,  # Specify column names to use when comparing duplicates\n        axis=Axis.ROW,\n        options={'keep': 'first'},  # Specify whether to keep 'first' or 'last' duplicate\n    )\n\n    return BaseAction(action).execute(df)\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/drops_duplicate_rows.py", "language": "python", "type": "transformer", "uuid": "drops_duplicate_rows"}, "transformers/remove_null_price.py:transformer:python:remove null price": {"content": "from mage_ai.data_cleaner.transformer_actions.base import BaseAction\nfrom mage_ai.data_cleaner.transformer_actions.constants import ActionType, Axis\nfrom mage_ai.data_cleaner.transformer_actions.utils import build_transformer_action\nfrom pandas import DataFrame\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef execute_transformer_action(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Remove rows where the price is null or exceeds 250 million EUR.\n    \"\"\"\n    # Define the maximum price limit\n    max_price = 250_000_000\n\n    df = df[\n            df['price'].notnull() & \n            (df['price'] <= max_price) & \n            (df['district'] != 'Z - Fora de Portugal')\n        ].reset_index(drop=True)\n\n    return df\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Test to ensure no rows have a null price or a price over 250 million EUR.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n    assert not output['price'].isnull().any(), 'There are rows with null prices in the output.'\n    assert (output['price'] <= 250_000_000).all(), 'There are rows with a price exceeding 250 million EUR.'\n", "file_path": "transformers/remove_null_price.py", "language": "python", "type": "transformer", "uuid": "remove_null_price"}, "transformers/cleans_dataframe.py:transformer:python:cleans dataframe": {"content": "from mage_ai.data_cleaner.transformer_actions.base import BaseAction\nfrom mage_ai.data_cleaner.transformer_actions.constants import ActionType, Axis\nfrom mage_ai.data_cleaner.transformer_actions.utils import build_transformer_action\nfrom pandas import DataFrame\nfrom datetime import datetime\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef execute_transformer_action(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Clean column names, add a 'unique_id' column, and move it to the first position.\n    \"\"\"\n    # Step 1: Clean column names\n    action = build_transformer_action(\n        df,\n        action_type=ActionType.CLEAN_COLUMN_NAME,\n        arguments=df.columns,\n        axis=Axis.COLUMN,\n    )\n    df = BaseAction(action).execute(df)\n\n    # Step 2: Rename '_type' to 'type' if it exists\n    if '_type' in df.columns:\n        print(\"Renaming '_type' to 'type'...\")\n        df.rename(columns={'_type': 'type'}, inplace=True)\n\n    # Step 3: Add 'unique_id' column if not already present\n    if 'unique_id' not in df.columns:\n        print(\"Adding 'unique_id' column...\")\n        def generate_unique_id(row):\n            # Numbers: Use the full value\n            total_area = str(row.get('totalarea', ''))\n\n            # Letters: Use the first letter of each word\n            floor = ''.join([word[0] for word in str(row.get('floor', '')).split()])\n            district = ''.join([word[0] for word in str(row.get('district', '')).split()])\n            city = ''.join([word[0] for word in str(row.get('city', '')).split()])\n            town = ''.join([word[0] for word in str(row.get('town', '')).split()])\n            property_type = ''.join([word[0] for word in str(row.get('type', '')).split()])\n\n            # Combine all parts into a unique_id without hyphens\n            return f\"{floor}{total_area}{district}{city}{town}{property_type}\"\n\n        df['unique_id'] = df.apply(generate_unique_id, axis=1)\n\n    # Step 4: Move 'unique_id' to the first column\n    columns = ['unique_id'] + [col for col in df.columns if col != 'unique_id']\n    df = df[columns]\n\n    # Steop 4: Add a push data column\n    df['push_date'] = datetime.utcnow().date()  # Add today's UTC date\n\n\n    # Step 5: Return the cleaned DataFrame\n    return df\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n    assert output.columns[0] == 'unique_id', \"'unique_id' column is not the first column in the output.\"\n    assert 'type' in output.columns, \"'type' column is missing from the output.\"\n    assert '_type' not in output.columns, \"'_type' column still exists in the output.\"", "file_path": "transformers/cleans_dataframe.py", "language": "python", "type": "transformer", "uuid": "cleans_dataframe"}, "pipelines/test_autentications/metadata.yaml:pipeline:yaml:test autentications/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - gcs_tester\n  - postgres_tester\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: api_test_call\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: api_test_call\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: GCS tester\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - api_test_call\n  uuid: gcs_tester\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: postgres_tester\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - api_test_call\n  uuid: postgres_tester\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-01-15 10:04:02.902487+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: test_autentications\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: test_autentications\nvariables_dir: /home/src/mage_data/de-zoomcamp-project\nwidgets: []\n", "file_path": "pipelines/test_autentications/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "test_autentications/metadata"}, "pipelines/real_estate/triggers.yaml:pipeline:yaml:real estate/triggers": {"content": "triggers:\n- description: null\n  envs: []\n  last_enabled_at: 2025-01-25 15:16:39.537848\n  name: weekly_load_real_estate_data\n  pipeline_uuid: real_estate\n  schedule_interval: '@weekly'\n  schedule_type: time\n  settings: null\n  sla: null\n  start_time: 2025-01-12 05:00:00\n  status: active\n  token: ${SLACK_TRIGGER_TOKEN}\n  variables: {}\n", "file_path": "pipelines/real_estate/triggers.yaml", "language": "yaml", "type": "pipeline", "uuid": "real_estate/triggers"}, "pipelines/real_estate/__init__.py:pipeline:python:real estate/  init  ": {"content": "", "file_path": "pipelines/real_estate/__init__.py", "language": "python", "type": "pipeline", "uuid": "real_estate/__init__"}, "pipelines/real_estate/metadata.yaml:pipeline:yaml:real estate/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_loaders/kaggle_loader.py\n    file_source:\n      path: data_loaders/kaggle_loader.py\n  downstream_blocks:\n  - cleans_dataframe\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: kaggle_loader\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: kaggle_loader\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - remove_null_price\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: cleans dataframe\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - kaggle_loader\n  uuid: cleans_dataframe\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - drops_duplicate_rows\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: remove null price\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - cleans_dataframe\n  uuid: remove_null_price\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - data_exporter_weely_postgres\n  - tablecomparison\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: drops_duplicate_rows\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - remove_null_price\n  uuid: drops_duplicate_rows\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file_path: data_exporters/data_exporter_weely_postgres.py\n    file_source:\n      path: data_exporters/data_exporter_weely_postgres.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: data_exporter_weely_postgres\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - drops_duplicate_rows\n  uuid: data_exporter_weely_postgres\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - tablecomparison\n  - data_evaluator\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: loads_real_estate_table\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: loads_real_estate_table\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - export_real_estate_data\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: tablecomparison\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - drops_duplicate_rows\n  - loads_real_estate_table\n  uuid: tablecomparison\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - dbt/models/staging/stg_staging__real_estate_data\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: export_real_estate_data\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - tablecomparison\n  uuid: export_real_estate_data\n- all_upstream_blocks_executed: true\n  color: teal\n  configuration:\n    file_source:\n      path: custom/data_evaluator.py\n  downstream_blocks:\n  - data_exporter_stats\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: data_evaluator\n  retry_config: null\n  status: executed\n  timeout: null\n  type: custom\n  upstream_blocks:\n  - loads_real_estate_table\n  uuid: data_evaluator\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_exporters/data_exporter_stats.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: data_exporter_stats\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - data_evaluator\n  uuid: data_exporter_stats\n- all_upstream_blocks_executed: false\n  color: pink\n  configuration:\n    file_path: custom/webhook_slack.py\n    file_source:\n      path: custom/webhook_slack.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: webhook slack\n  retry_config: null\n  status: executed\n  timeout: null\n  type: custom\n  upstream_blocks:\n  - dbt/models/core/real_estate_data_w_geolocation\n  uuid: webhook_slack\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dbt_project_name: dbt\n    file_path: dbt/models/core/real_estate_data_w_geolocation.sql\n    file_source:\n      path: dbt/models/core/real_estate_data_w_geolocation.sql\n      project_path: dbt\n  downstream_blocks:\n  - webhook_slack\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: real_estate_data_w_geolocation\n  retry_config: null\n  status: executed\n  timeout: null\n  type: dbt\n  upstream_blocks:\n  - dbt/models/staging/stg_cleaned_real_estate_data\n  uuid: dbt/models/core/real_estate_data_w_geolocation\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dbt_project_name: dbt\n    file_path: dbt/models/staging/stg_cleaned_real_estate_data.sql\n    file_source:\n      path: dbt/models/staging/stg_cleaned_real_estate_data.sql\n      project_path: dbt\n  downstream_blocks:\n  - dbt/models/core/real_estate_data_w_geolocation\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/models/staging/stg_cleaned_real_estate_data\n  retry_config: null\n  status: executed\n  timeout: null\n  type: dbt\n  upstream_blocks:\n  - dbt/models/staging/stg_staging__real_estate_data\n  uuid: dbt/models/staging/stg_cleaned_real_estate_data\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dbt_project_name: dbt\n    file_path: dbt/models/staging/stg_staging__real_estate_data.sql\n    file_source:\n      path: dbt/models/staging/stg_staging__real_estate_data.sql\n      project_path: dbt\n  downstream_blocks:\n  - dbt/models/staging/stg_cleaned_real_estate_data\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/models/staging/stg_staging__real_estate_data\n  retry_config: null\n  status: executed\n  timeout: null\n  type: dbt\n  upstream_blocks:\n  - export_real_estate_data\n  uuid: dbt/models/staging/stg_staging__real_estate_data\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals:\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: golden ronin\n  retry_config: null\n  status: updated\n  timeout: null\n  type: conditional\n  upstream_blocks:\n  - dbt/models/core/real_estate_data_w_geolocation\n  - dbt/models/staging/stg_cleaned_real_estate_data\n  - dbt/models/staging/stg_staging__real_estate_data\n  uuid: golden_ronin\ncreated_at: '2025-01-16 07:53:09.712262+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: real_estate\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: real_estate\nvariables_dir: /home/src/mage_data/de-zoomcamp-project\nwidgets: []\n", "file_path": "pipelines/real_estate/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "real_estate/metadata"}, "/home/src/de-zoomcamp-project/custom/webhook_slack.py:custom:python:home/src/de-zoomcamp-project/custom/webhook slack": {"content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\nimport requests\nimport os\nfrom dotenv import load_dotenv\nfrom mage_ai.data_preparation.repo_manager import get_repo_path\nfrom mage_ai.data_preparation.variable_manager import VariableManager\n\n# Load .env file\nload_dotenv()\n\n\ndef notify_slack(pipeline_name, status, db_data=None, dbt_status=None):\n    \"\"\"\n    Sends a notification to Slack with the pipeline status and DBT execution information.\n    \"\"\"\n    # Access the webhook URL from environment variables\n    webhook_url = os.getenv('SLACK_WEBHOOK_URL')\n\n    if not webhook_url:\n        raise ValueError(\"Slack webhook URL is not set in the environment variables.\")\n\n    # Prepare the message\n    if db_data and isinstance(db_data, int):\n        message_text = (\n            f\"Pipeline `{pipeline_name}` has completed with status: {status}.\\n\"\n            f\"Number of rows exported to the database: {db_data}\\n\"\n            f\"DBT execution status: {'Executed' if dbt_status else 'Skipped'}.\"\n        )\n    else:\n        message_text = (\n            f\"Pipeline `{pipeline_name}` has completed with status: {status}.\\n\"\n            f\"No data was exported.\\n\"\n            f\"DBT execution status: {'Executed' if dbt_status else 'Skipped'}.\"\n        )\n\n    message = {\"text\": message_text}\n\n    # Send the Slack message\n    response = requests.post(webhook_url, json=message)\n\n    # Debug: Print the response\n    print(f\"Response status code: {response.status_code}\")\n    print(f\"Response text: {response.text}\")\n\n\n@custom\ndef transform_custom(*args, **kwargs):\n    \"\"\"\n    Sends a Slack notification with the number of rows exported and DBT status.\n    \"\"\"\n    # Retrieve the DataFrame from the upstream block, if available\n    df_data = args[0] if args else None\n\n    # Calculate the number of rows exported\n    num_rows_exported = len(df_data) if df_data is not None else 0\n\n    # Debug: Print the row count\n    print(f\"Number of rows exported: {num_rows_exported}\")\n\n    # Check the global variable `has_new_rows` to determine if DBT was run\n    variable_manager = VariableManager(get_repo_path())\n    try:\n        has_new_rows = variable_manager.get_variable('default', 'has_new_rows', 'has_new_rows')\n    except Exception as e:\n        print(f\"Error fetching 'has_new_rows': {e}\")\n        has_new_rows = False\n\n    # Notify Slack\n    notify_slack(\n        pipeline_name=\"data_real_estate\",\n        status=\"Success\",\n        db_data=num_rows_exported,\n        dbt_status=has_new_rows\n    )\n\n    return num_rows_exported\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "/home/src/de-zoomcamp-project/custom/webhook_slack.py", "language": "python", "type": "custom", "uuid": "webhook_slack"}}, "custom_block_template": {"custom_templates/blocks/condition_has_rows:conditional:python": {"block_type": "conditional", "color": null, "configuration": null, "description": null, "language": "python", "name": null, "pipeline": {}, "tags": [], "user": {}, "template_uuid": "condition_has_rows", "uuid": "custom_templates/blocks/condition_has_rows", "content": "if 'condition' not in globals():\n    from mage_ai.data_preparation.decorators import condition\nfrom mage_ai.data_preparation.repo_manager import get_repo_path\nfrom mage_ai.data_preparation.variable_manager import VariableManager\n\n\n@condition\ndef evaluate_condition(*args, **kwargs) -> bool:\n    \"\"\"\n    Evaluate whether the DBT block should run based on the presence of new rows.\n    \"\"\"\n    # Access the global variable to check if there are new rows\n    variable_manager = VariableManager(get_repo_path())\n    has_new_rows = variable_manager.get_variable('default', 'has_new_rows')\n\n    # Return True if there are new rows, False otherwise\n    return has_new_rows\n"}}, "mage_template": {"data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}